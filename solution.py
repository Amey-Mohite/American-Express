# -*- coding: utf-8 -*-
"""Solution.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1J2AyF0t_2v4CcEXE9SVTW2DbBCbskAlY
"""

from google.colab import drive
drive.mount('/content/gdrive')

import pandas as pd 
from sklearn.feature_selection import VarianceThreshold
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn import preprocessing
from sklearn.model_selection import train_test_split as tts
from sklearn.metrics import r2_score,mean_squared_error,mean_squared_log_error
from math import sqrt
from keras.callbacks import ModelCheckpoint
from keras.models import Sequential
from keras.layers import Dense, Activation, Flatten,Dropout 
from matplotlib import pyplot as plt
import seaborn as sb
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import warnings 
warnings.filterwarnings('ignore')
warnings.filterwarnings('ignore', category=DeprecationWarning)

import pandas as pd
import numpy as np
from scipy import stats
import seaborn as sns
import matplotlib.pyplot as plt

train = pd.read_csv("/content/gdrive/My Drive/American Express /train.csv")
test = pd.read_csv("/content/gdrive/My Drive/American Express /test.csv")

train_data =  train.copy()
test_data = test.copy()

id1 = test_data['id']

y = train_data['cc_cons']

len_train=len(train_data)
len_test=len(test_data)

train_data = train_data.drop(['cc_cons'],1)

data = pd.concat([train_data,test_data],axis=0)

data.shape

data.isnull().sum()

data = data.drop(['dc_cons_apr','dc_cons_may','dc_cons_jun','dc_count_apr','dc_count_may','dc_count_jun','personal_loan_active','vehicle_loan_active','personal_loan_closed','vehicle_loan_closed','investment_1','investment_2','investment_3','investment_4','loan_enq'],axis=1)

data.shape

data.isnull().sum()

def mean_data(x):
  data[x]=data[x].fillna(data[x].mean())

def mode_data(x):
  data[x]=data[x].fillna(int(data[x].mode()))

mean_data('debit_amount_apr')
mean_data('credit_amount_apr')
mean_data('max_credit_amount_apr')
mean_data('debit_amount_may')
mean_data('credit_amount_may')
mean_data('max_credit_amount_may')
mean_data('debit_amount_jun')
mean_data('credit_amount_jun')
mean_data('max_credit_amount_jun')

mode_data('cc_count_apr')
mode_data('cc_count_may')
mode_data('cc_count_jun')
mode_data('card_lim')
mode_data('debit_count_apr')
mode_data('credit_count_apr')
mode_data('credit_count_may')
mode_data('debit_count_may')
mode_data('credit_count_jun')
mode_data('debit_count_jun')

def out(x):
  data.boxplot(column = [x],return_type='axes')
  Q1 = data[x].quantile(0.25)
  Q3 = data[x].quantile(0.75)
  IQR = Q3 - Q1
  print(IQR)
  max1=Q3+1.5*IQR
  min1=Q1-1.5*IQR
  data.loc[data1[x]>max1,[x]]=max1
  data.loc[data1[x]<min1,[x]]=min1
  data.boxplot(column = [x],return_type='axes')

data.isnull().sum()

data1 = data.copy()

data1 = data1.drop(['id'],axis=1)

c = data1.columns

c=c[2:]

c

for i in range(len(c)):
  out(c[i])

data1.head(4)

train_data_final = data1[:len_train]

train_data_final.tail(4)

test_data_final = data1[len_train:]

test_data_final.head(4)

len(train_data_final),len(y)

scaler=StandardScaler()

numerical_data=train_data_final.select_dtypes(include=np.number)
cols = numerical_data.columns
numerical_data=scaler.fit_transform(numerical_data)
numerical_data = pd.DataFrame(numerical_data,columns=cols)

le = preprocessing.LabelEncoder()

categorical_data = train_data_final.select_dtypes(include=np.object)
categorical_data['account_type']=le.fit_transform(categorical_data['account_type'])
categorical_data['gender']=le.fit_transform(categorical_data['gender'])

train_data_final = pd.concat([numerical_data,categorical_data],axis=1)

train_data_final.head(5)

X_train, X_test, y_train, y_test = tts(train_data_final,y,test_size=0.3,random_state=0)
X_train.shape, X_test.shape

NN_model = Sequential()

# The Input Layer :
NN_model.add(Dense(512, kernel_initializer='normal',input_dim = X_train.shape[1], activation='relu'))

# The Hidden Layers :
NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))
NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))
NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))
NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))
# The Output Layer :
NN_model.add(Dense(1, kernel_initializer='normal',activation='linear'))

# Compile the network :
NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])
NN_model.summary()

checkpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5' 
checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 3, save_best_only = True, mode ='auto')
callbacks_list = [checkpoint]

NN_model.fit(X_train, y_train, epochs=50, batch_size=16, validation_split = 0.2, callbacks=callbacks_list)

wights_file = '/content/Weights-010--4980.93799.hdf5' # choose the best checkpoint 
NN_model.load_weights(wights_file) # load it
NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])

pred = NN_model.predict(X_test)

r2_score(y_test,pred)

mean_squared_error(y_test,pred)

sqrt(mean_squared_log_error(y_test,pred))*100

def make_submission(prediction, sub_name):
  my_submission = pd.DataFrame({'Id':pd.read_csv('test.csv').Id,'SalePrice':prediction})
  my_submission.to_csv('{}.csv'.format(sub_name),index=False)
  print('A submission file has been made')

predictions = NN_model.predict(test)
make_submission(predictions[:,0],'submission(NN).csv')